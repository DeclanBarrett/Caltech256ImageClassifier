{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T07:03:17.630097Z",
     "iopub.status.busy": "2022-06-10T07:03:17.629820Z",
     "iopub.status.idle": "2022-06-10T07:03:17.636141Z",
     "shell.execute_reply": "2022-06-10T07:03:17.635450Z",
     "shell.execute_reply.started": "2022-06-10T07:03:17.630071Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.utils import compute_class_weight\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tarfile\n",
    "\n",
    "import functools\n",
    "import pathlib\n",
    "import shutil\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "CALTECH_256_DIR = '256_ObjectCategories'\n",
    "CALTECH_101_DIR = '101_ObjectCategories'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras.backend as K\n",
    "from keras import layers\n",
    "from keras import activations\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "#from keras.utils import to_categorical\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from random import seed\n",
    "from random import random\n",
    "seed(1)\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T07:03:17.637732Z",
     "iopub.status.busy": "2022-06-10T07:03:17.637484Z",
     "iopub.status.idle": "2022-06-10T07:03:18.428282Z",
     "shell.execute_reply": "2022-06-10T07:03:18.427491Z",
     "shell.execute_reply.started": "2022-06-10T07:03:17.637701Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "! cd 256_ObjectCategories/train && find . -mindepth 1 -maxdepth 1 -type d | grep 101 | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T07:03:18.429762Z",
     "iopub.status.busy": "2022-06-10T07:03:18.429565Z",
     "iopub.status.idle": "2022-06-10T07:03:18.433759Z",
     "shell.execute_reply": "2022-06-10T07:03:18.433105Z",
     "shell.execute_reply.started": "2022-06-10T07:03:18.429737Z"
    }
   },
   "outputs": [],
   "source": [
    "image_size_y = 64\n",
    "image_size_x = 64\n",
    "output_size = 257\n",
    "image_depth = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T07:03:18.435653Z",
     "iopub.status.busy": "2022-06-10T07:03:18.435471Z",
     "iopub.status.idle": "2022-06-10T07:03:18.465620Z",
     "shell.execute_reply": "2022-06-10T07:03:18.464764Z",
     "shell.execute_reply.started": "2022-06-10T07:03:18.435631Z"
    }
   },
   "outputs": [],
   "source": [
    "def download(url: str, filename: str):\n",
    "    \"\"\" Util function for downloading file with a loading bar and saves to folder\n",
    "    \n",
    "    \n",
    "    Args:\n",
    "        url: url to fetch\n",
    "        filename: file name to save data to\n",
    "    \"\"\"    \n",
    "    r = requests.get(url, stream=True, allow_redirects=True)\n",
    "    if r.status_code != 200:\n",
    "        r.raise_for_status()  # Will only raise for 4xx codes, so...\n",
    "        raise RuntimeError(f\"Request to {url} returned status code {r.status_code}\")\n",
    "    file_size = int(r.headers.get(\"Content-Length\", 0))\n",
    "\n",
    "    path = pathlib.Path(filename).expanduser().resolve()\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    desc = \"(Unknown total file size)\" if file_size == 0 else \"\"\n",
    "    r.raw.read = functools.partial(\n",
    "        r.raw.read, decode_content=True\n",
    "    )  # Decompress if needed\n",
    "    with tqdm.wrapattr(r.raw, \"read\", total=file_size, desc=desc) as r_raw:\n",
    "        with path.open(\"wb\") as f:\n",
    "            shutil.copyfileobj(r_raw, f)\n",
    "\n",
    "    return path\n",
    "\n",
    "\n",
    "def fetch_caltech256():\n",
    "    \"\"\"Fetches caltech 256 dataset and extracts to 256_ObjectCategories\"\"\"\n",
    "    if not os.path.isfile(\"data.tar\"):\n",
    "        download(\n",
    "            \"https://data.caltech.edu/tindfiles/serve/813641b9-cb42-4e21-9da5-9d24a20bb4a4\",\n",
    "            \"data.tar\",\n",
    "        )\n",
    "    if not os.path.isdir(\"256_ObjectCategories\"):\n",
    "        with tarfile.open(\"data.tar\") as tar:\n",
    "            print('Extracting data tar...')\n",
    "            tar.extractall()\n",
    "        if os.path.isdir(CALTECH_256_DIR + '/056.dog/greg'):\n",
    "            os.rmdir(CALTECH_256_DIR + '/056.dog/greg/vision309')\n",
    "            os.rmdir(CALTECH_256_DIR + '/056.dog/greg/')\n",
    "\n",
    "def fetch_caltech101():\n",
    "    \"\"\"Fetches caltech 256 dataset and extracts to 256_ObjectCategories\"\"\"\n",
    "    if not os.path.isfile(\"data.zip\"):\n",
    "        download(\n",
    "            \"https://data.caltech.edu/tindfiles/serve/e41f5188-0b32-41fa-801b-d1e840915e80/\",\n",
    "            \"data.zip\",\n",
    "        )\n",
    "    if not os.path.isdir(\"caltech-101\"):\n",
    "        with zipfile.ZipFile('data.zip', 'r') as zip_:\n",
    "            print('Extracting zip...')\n",
    "            zip_.extractall()\n",
    "    if not os.path.isdir(CALTECH_101_DIR):\n",
    "        with tarfile.open(\"caltech-101/101_ObjectCategories.tar.gz\") as tar:\n",
    "            print('Extracting data tar...')\n",
    "            tar.extractall()\n",
    "        with tarfile.open(\"caltech-101/Annotations.tar\") as tar:\n",
    "            print('Extracting annotations tar...')\n",
    "            tar.extractall()\n",
    "            print('Done')\n",
    "\n",
    "def create_dataset(data_dir: str):\n",
    "    \"\"\"Creates a tensroflow dataset that preprocesses images.\n",
    "    \n",
    "    Preprocessing done is: \n",
    "      * resizing images to 128x128 \n",
    "      * add padding\n",
    "    \"\"\"\n",
    "    class_names = np.array([\"\"] + sorted(os.listdir(data_dir)))\n",
    "\n",
    "    def parse_image(filename):\n",
    "        parts = tf.strings.split(filename, os.sep)\n",
    "        label = parts[-2]\n",
    "        oneHot = label == class_names\n",
    "        encodedLabel = tf.argmax(oneHot)\n",
    "\n",
    "        image = tf.io.read_file(filename)\n",
    "        image = tf.io.decode_jpeg(image, channels=3)\n",
    "        image = tf.image.resize(image, size=(image_size_y, image_size_x)) #64\n",
    "        #image = tf.image.resize_with_pad(image, target_height=image_size_y, target_width=image_size_x)\n",
    "        image = image / 256\n",
    "\n",
    "        return image, encodedLabel\n",
    "\n",
    "    list_ds = tf.data.Dataset.list_files(data_dir + \"/*/*.jpg\", shuffle=True)\n",
    "\n",
    "    return list_ds.map(parse_image)\n",
    "\n",
    "\n",
    "def move_files(files: list, destination_folder: str):\n",
    "    \"\"\"Helper file to move files arouond\"\"\"\n",
    "    if not os.path.isdir(destination_folder):\n",
    "        os.makedirs(destination_folder)\n",
    "\n",
    "    for file_ in files:\n",
    "        destination = os.path.join(destination_folder, os.path.basename(file_))\n",
    "        os.rename(file_, destination)\n",
    "\n",
    "\n",
    "def split_data(data_dir):\n",
    "    print('Splitting data...')\n",
    "    for folder, folders, files in list(os.walk(data_dir))[1:]:\n",
    "        files.sort()\n",
    "\n",
    "        files = [os.path.join(folder, file_) for file_ in files]\n",
    "\n",
    "        n_files = len(files)\n",
    "        n_train = n_files // 2\n",
    "        n_val = n_files // 4\n",
    "        n_test = n_files - n_train - n_val\n",
    "\n",
    "        i_train = 0\n",
    "        i_val = i_train + n_train\n",
    "        i_test = i_val + n_val\n",
    "\n",
    "        move_files(\n",
    "            files[:i_val], os.path.join(data_dir, \"train\", os.path.basename(folder))\n",
    "        )\n",
    "        move_files(\n",
    "            files[i_val:i_test], os.path.join(data_dir, \"val\", os.path.basename(folder))\n",
    "        )\n",
    "        move_files(\n",
    "            files[i_test:], os.path.join(data_dir, \"test\", os.path.basename(folder))\n",
    "        )\n",
    "\n",
    "        if not os.listdir(folder):\n",
    "            os.rmdir(folder)\n",
    "\n",
    "\n",
    "def create_datasets(is_101=False):\n",
    "    if is_101:\n",
    "        fetch_caltech101()\n",
    "        data_dir = CALTECH_101_DIR\n",
    "    else:\n",
    "        fetch_caltech256()\n",
    "        data_dir = CALTECH_256_DIR\n",
    "\n",
    "    print('Loading data...')\n",
    "\n",
    "    if not os.path.isdir(data_dir + \"/train\"):\n",
    "        split_data(data_dir)\n",
    "\n",
    "    train_x, train_y = zip(*create_dataset(data_dir + \"/train\").as_numpy_iterator())\n",
    "    val_x, val_y = zip(*create_dataset(data_dir + \"/val\").as_numpy_iterator())\n",
    "    test_x, test_y = zip(*create_dataset(data_dir + \"/test\").as_numpy_iterator())\n",
    "    print('Done')\n",
    "\n",
    "    return train_x, train_y, val_x, val_y, test_x, test_y\n",
    "\n",
    "def create_datasets_tf(is_101=False):\n",
    "    if is_101:\n",
    "        fetch_caltech101()\n",
    "        data_dir = CALTECH_101_DIR\n",
    "    else:\n",
    "        fetch_caltech256()\n",
    "        data_dir = CALTECH_256_DIR\n",
    "\n",
    "    if not os.path.isdir(data_dir + \"/train\"):\n",
    "        split_data()\n",
    "\n",
    "    train = create_dataset(data_dir + \"/train\")\n",
    "    val = create_dataset(data_dir + \"/val\")\n",
    "    test = create_dataset(data_dir + \"/test\")\n",
    "\n",
    "    return train, val, test\n",
    "\n",
    "def get_label_name(label: int, is_101=False):\n",
    "    \"\"\"Gets the name associated with a label\"\"\" \n",
    "    if is_101:\n",
    "        data_dir = CALTECH_101_DIR\n",
    "    else:\n",
    "        data_dir = CALTECH_256_DIR\n",
    "\n",
    "    dirs = os.listdir(data_dir + \"/train\")\n",
    "    return sorted(dirs)[label - 1].split(\".\")[1]\n",
    "\n",
    "def get_label_for_name(name: str, is_101=False):\n",
    "    if is_101:\n",
    "        data_dir = CALTECH_101_DIR\n",
    "    else:\n",
    "        data_dir = CALTECH_256_DIR\n",
    "\n",
    "    search_list = [f.split('.')[1] for f in sorted(os.listdir(data_dir  + '/train'))]\n",
    "    return search_list.index(name) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T07:03:18.467309Z",
     "iopub.status.busy": "2022-06-10T07:03:18.467115Z",
     "iopub.status.idle": "2022-06-10T07:03:18.480098Z",
     "shell.execute_reply": "2022-06-10T07:03:18.479153Z",
     "shell.execute_reply.started": "2022-06-10T07:03:18.467284Z"
    }
   },
   "outputs": [],
   "source": [
    "# Class for the data generator with autoencoder generator output\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    \n",
    "    # Input arguments are as follows:\n",
    "    #  x:            our X data array that we'll augment\n",
    "    #  y_one:        y labels for the first output\n",
    "    #  y_two:        y labels for the second ouput\n",
    "    #  data_aug:     our data augmentor\n",
    "    #  batch_size:   the batch size to return from the generator\n",
    "    #  dim:          size of images\n",
    "    #  n_channels:   number of image channels\n",
    "    #  shuffle:      flag to indicate if we should shuffle the data at the end of the epoch\n",
    "    def __init__(self, x, y_one, y_two, data_aug,                \n",
    "                 batch_size=32, dim=(64,64), n_channels=3,\n",
    "                 shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.x = x\n",
    "        self.y_one = y_one\n",
    "        self.y_two = y_two\n",
    "        self.data_aug = data_aug\n",
    "        self.list_IDs = np.arange(0, self.x.shape[0])\n",
    "        self.n_channels = n_channels\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "            \n",
    "\n",
    "    # Function to generate data. This will take in a list of sample indicies, and then for each\n",
    "    # these apply augmentation, and return the augmented data and labels\n",
    "    #\n",
    "    # If you wish to check the mechanics out in more detail, you can uncomment the two print lines\n",
    "    # which will show you the IDs that are being manipulated and help show what's going on - however\n",
    "    # this will also generate a lot of output text during model training.\n",
    "    #\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization - creating space for X and y data\n",
    "#        print(list_IDs_temp)\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        # creating a list of empty arrays to hold our data labels. \n",
    "        # would initialise this for the number of output labels we have\n",
    "        y = [np.empty((self.batch_size), dtype=int),\n",
    "             np.empty((self.batch_size), dtype=int),\n",
    "             np.empty((self.batch_size, *self.dim, self.n_channels))]\n",
    "\n",
    "        # Generate data, loop through the list of IDs we have and generate data for each in turn\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            #print(ID)\n",
    "            randomNumber = int(999999999 * random())\n",
    "            # apply random transformation based of our datagen instance\n",
    "            X[i,] = self.data_aug.random_transform(self.x[ID,], randomNumber)\n",
    "            # copy our y labels across\n",
    "            y[0][i,] = self.y_one[ID]\n",
    "            y[1][i,] = self.y_two[ID]\n",
    "            y[2][i,] = X[i,]\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T07:03:18.481471Z",
     "iopub.status.busy": "2022-06-10T07:03:18.481293Z",
     "iopub.status.idle": "2022-06-10T07:03:37.759328Z",
     "shell.execute_reply": "2022-06-10T07:03:37.758659Z",
     "shell.execute_reply.started": "2022-06-10T07:03:18.481448Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, val_x, val_y, test_x, test_y = create_datasets(is_101=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T07:03:37.762104Z",
     "iopub.status.busy": "2022-06-10T07:03:37.761892Z",
     "iopub.status.idle": "2022-06-10T07:03:37.769187Z",
     "shell.execute_reply": "2022-06-10T07:03:37.767895Z",
     "shell.execute_reply.started": "2022-06-10T07:03:37.762080Z"
    }
   },
   "outputs": [],
   "source": [
    "#for i in range(257):\n",
    "#    print(str(i) + \" is \" + get_label_name(i))\n",
    "\n",
    "ignore = [0, 82, 159]\n",
    "plant = [15, 25, 68, 92, 103, 147, 154, 204, 221, 242]\n",
    "friend = [40, 159, 253]\n",
    "\n",
    "new_classes = ['ignore', 'plant', 'friend', 'garbage']\n",
    "\n",
    "def select_garbage(data): \n",
    "    new_data = []\n",
    "    for x in data:\n",
    "        converted_to = 3\n",
    "        if x in ignore:\n",
    "            converted_to = 0\n",
    "        elif x in plant:\n",
    "            converted_to = 1\n",
    "        elif x in friend:\n",
    "            converted_to = 2\n",
    "        \n",
    "        new_data.append(converted_to)\n",
    "    #print(str(x) + \" is now \" + str())\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T07:03:37.770356Z",
     "iopub.status.busy": "2022-06-10T07:03:37.770167Z",
     "iopub.status.idle": "2022-06-10T07:03:38.682311Z",
     "shell.execute_reply": "2022-06-10T07:03:38.681510Z",
     "shell.execute_reply.started": "2022-06-10T07:03:37.770333Z"
    }
   },
   "outputs": [],
   "source": [
    "train_x_new = np.asarray(train_x)\n",
    "train_y_new = np.asarray(train_y)\n",
    "train_y_new = train_y_new-1\n",
    "train_garbage = np.asarray(select_garbage(train_y_new))\n",
    "val_x_new = np.asarray(val_x)\n",
    "val_y_new = np.asarray(val_y)\n",
    "val_y_new  = val_y_new -1\n",
    "val_garbage = np.asarray(select_garbage(val_y_new))\n",
    "test_x_new = np.asarray(test_x)\n",
    "test_y_new = np.asarray(test_y)\n",
    "test_y_new = test_y_new-1\n",
    "test_garbage = np.asarray(select_garbage(test_y_new))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T07:03:38.683486Z",
     "iopub.status.busy": "2022-06-10T07:03:38.683298Z",
     "iopub.status.idle": "2022-06-10T07:03:38.688423Z",
     "shell.execute_reply": "2022-06-10T07:03:38.687752Z",
     "shell.execute_reply.started": "2022-06-10T07:03:38.683464Z"
    }
   },
   "outputs": [],
   "source": [
    "def focal_loss(target, output, gamma=2):\n",
    "    output /= K.sum(output, axis=-1, keepdims=True)\n",
    "    eps = K.epsilon()\n",
    "    output = K.clip(output, eps, 1. - eps)\n",
    "    return -K.sum(K.pow(1. - output, gamma) * target * K.log(output), axis=-1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T07:03:38.691126Z",
     "iopub.status.busy": "2022-06-10T07:03:38.690770Z",
     "iopub.status.idle": "2022-06-10T07:03:38.697082Z",
     "shell.execute_reply": "2022-06-10T07:03:38.696142Z",
     "shell.execute_reply.started": "2022-06-10T07:03:38.691100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ad noted above: these params are not suitable, just used to demo it in practice\n",
    "data_aug = ImageDataGenerator(\n",
    "                            # rotate between -10, +10 degrees\n",
    "                            rotation_range=4,\n",
    "                            # horiziontal shift by +/- 10% of the image width\n",
    "                            width_shift_range=0.05,\n",
    "                            # vertical shift by +/- 10% of the image width\n",
    "                            height_shift_range=0.05,\n",
    "                            # +/- 10% range for randomly applying a shearing transform\n",
    "                            #shear_range=0.05,\n",
    "                            # +/- 10% range for zooming\n",
    "                            zoom_range=0.2,\n",
    "                            # allow horizontal flips of data\n",
    "                            #horizontal_flip=True,\n",
    "                            # what value to place in new pixels, given the nature of our data\n",
    "                            # extend images\n",
    "                            fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T07:03:38.698287Z",
     "iopub.status.busy": "2022-06-10T07:03:38.698002Z",
     "iopub.status.idle": "2022-06-10T07:03:38.708873Z",
     "shell.execute_reply": "2022-06-10T07:03:38.707987Z",
     "shell.execute_reply.started": "2022-06-10T07:03:38.698248Z"
    }
   },
   "outputs": [],
   "source": [
    "datagen = DataGenerator(train_x_new, train_y_new, train_garbage, data_aug, batch_size=64, dim=(image_size_y, image_size_x), n_channels=image_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVGGBlock(x, filter):\n",
    "     # run pairs of conv layers, all 3s3 kernels\n",
    "    x = layers.Conv2D(filters=filter, kernel_size=(3,3), padding='same', activation='relu')(x)\n",
    "    x = layers.Conv2D(filters=filter, kernel_size=(3,3), padding='same', activation=None)(x)\n",
    "    # batch normalisation, before the non-linearity\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    # spatial dropout, this will drop whole kernels, i.e. 20% of our 3x3 filters will be dropped out rather\n",
    "    # than dropping out 20% of the invidual pixels\n",
    "    x = layers.SpatialDropout2D(0.2)(x)\n",
    "    # max pooling, 2x2, which will downsample the image\n",
    "    x = layers.MaxPool2D(pool_size=(2, 2))(x)\n",
    "    return x\n",
    "\n",
    "def getFCCBlock(x, filter):\n",
    "    x = layers.Dense(filter, activation=None)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    return x\n",
    "\n",
    "#DCNN model needs to match the MNIST\n",
    "def get_DCNN_model(upscaling_bool=False, global_pool=False, filters=[16, 32, 64], fcd=[1024, 512], pretrained=False):\n",
    "    # our model, input in an image shape\n",
    "    input_t = keras.Input(shape=(image_size_y, image_size_x, image_depth))\n",
    "\n",
    "    if (upscaling_bool):\n",
    "        x = keras.layers.Lambda(lambda image: tf.image.resize(image, (224, 224)))(input_t)\n",
    "    else:\n",
    "        x = input_t\n",
    "\n",
    "    if (pretrained==False):\n",
    "        for filter_stage in filters:\n",
    "            x = getVGGBlock(x, filter_stage)\n",
    "    else:\n",
    "        x_model = keras.applications.VGG16(weights='imagenet', include_top=False, input_tensor=x)\n",
    "        for layer in x_model.layers[:-5]:\n",
    "            layer.trainable = False\n",
    "        x = x_model.output\n",
    "        \n",
    "    pre_x = x\n",
    "    # final conv2d, batch norm and spatial dropout\n",
    "    # flatten layer\n",
    "    if global_pool:\n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    # we'll use a couple of dense layers here, mainly so that we can show what another dropout layer looks like \n",
    "    # in the middle\n",
    "    x1 = x\n",
    "    for fcc in fcd:\n",
    "        x1 = getFCCBlock(x1, fcc)\n",
    "    output_class = layers.Dense(257, activation='softmax', name=\"class\")(x1)\n",
    "\n",
    "    x2 = x\n",
    "    for fcc in fcd:\n",
    "        x2 = getFCCBlock(x2, fcc)\n",
    "    output_garbage = layers.Dense(4, activation='softmax', name=\"garbage\")(x2)\n",
    "\n",
    "    x3 = layers.UpSampling2D((2, 2))(pre_x)\n",
    "    x3 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x3)\n",
    "    x3 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x3)\n",
    "    \n",
    "    x3 = layers.UpSampling2D((2, 2))(x3)\n",
    "    x3 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x3)\n",
    "    x3 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x3)\n",
    "    \n",
    "    x3 = layers.UpSampling2D((2, 2))(x3)\n",
    "    x3 = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(x3)\n",
    "    x3 = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(x3)\n",
    "    x3 = layers.Conv2D(3, (1, 1), activation='softmax', padding='same')(x3)\n",
    "    x3 = keras.layers.Lambda(lambda image: tf.image.resize(image, (image_size_y, image_size_x)))(x3)\n",
    "    output_decoded = layers.Conv2D(3, (1, 1), activation='softmax', padding='same', name=\"decoded\")(x3)\n",
    "    # the output\n",
    "    model = keras.Model(inputs=input_t, outputs=[output_class, output_garbage, output_decoded], name='Ass1cQ2')\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T07:03:38.724950Z",
     "iopub.status.busy": "2022-06-10T07:03:38.724755Z",
     "iopub.status.idle": "2022-06-10T07:03:38.731503Z",
     "shell.execute_reply": "2022-06-10T07:03:38.730616Z",
     "shell.execute_reply.started": "2022-06-10T07:03:38.724925Z"
    }
   },
   "outputs": [],
   "source": [
    "def update_trainable_layers(model, layers_to_train_local, norm_on=False):\n",
    "    #pretrained_model.summary()\n",
    "    for layer in model.layers[-21].layers:\n",
    "        layer.trainable = False\n",
    "    if (layers_to_train_local != 0):\n",
    "        for layer in model.layers[-21].layers[-layers_to_train_local:]:\n",
    "            layer.trainable = True\n",
    "            if (norm_on == False):\n",
    "                if (isinstance(layer, layers.BatchNormalization)):\n",
    "                    layer.trainable = False\n",
    "    for i, layer in enumerate(model.layers[-21].layers):\n",
    "        if layer.trainable:\n",
    "            print(i, layer.name, \"-\", layer.trainable)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T07:03:38.732610Z",
     "iopub.status.busy": "2022-06-10T07:03:38.732438Z",
     "iopub.status.idle": "2022-06-10T07:03:38.740796Z",
     "shell.execute_reply": "2022-06-10T07:03:38.739593Z",
     "shell.execute_reply.started": "2022-06-10T07:03:38.732589Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model_history, trained_model):\n",
    "    fig = plt.figure(figsize=[10, 10])\n",
    "    ax = fig.add_subplot(1, 3, 1)\n",
    "    predict_class, predict_garbage, predict_picture = trained_model.predict(test_x_new);\n",
    "    garbage_indexes = tf.argmax(predict_garbage, axis=1)\n",
    "    class_indexes = tf.argmax(predict_class, axis=1)\n",
    "    cm = confusion_matrix(test_garbage, garbage_indexes)\n",
    "    c = ConfusionMatrixDisplay(cm, display_labels=range(4))\n",
    "    c.plot(ax = ax)    \n",
    "    ax.set_title('Testing Loss');\n",
    "    ax = fig.add_subplot(1, 3, 2)\n",
    "    ax.plot(model_history.history['loss'], label=\"Training Loss\")\n",
    "    ax.plot(model_history.history['val_loss'], label=\"Validation Loss\")\n",
    "    ax.plot(model_history.history['val_garbage_loss'], label=\"Validation Garbage Loss\")\n",
    "    ax.plot(model_history.history['val_class_loss'], label=\"Validation Class Loss\")\n",
    "    ax.legend()\n",
    "    ax.set_title('Testing Accuracy');\n",
    "    ax = fig.add_subplot(1, 3, 3)\n",
    "    #ax.plot(model_history.history['accuracy'], label=\"Training Accuracy\")\n",
    "    #ax.plot(model_history.history['val_accuracy'], label=\"Validation Accuracy\")\n",
    "    ax.plot(model_history.history['val_garbage_accuracy'], label=\"Validation Garbage Accuracy\")\n",
    "    ax.plot(model_history.history['val_class_accuracy'], label=\"Validation Class Accuracy\")\n",
    "    ax.legend()\n",
    "    print(classification_report(test_garbage, garbage_indexes))\n",
    "    print(classification_report(test_y_new, class_indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T07:03:38.742326Z",
     "iopub.status.busy": "2022-06-10T07:03:38.742066Z",
     "iopub.status.idle": "2022-06-10T07:03:38.750935Z",
     "shell.execute_reply": "2022-06-10T07:03:38.750036Z",
     "shell.execute_reply.started": "2022-06-10T07:03:38.742291Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_model(iteration, model, local_epoch=150, batch_size_chosen=64):  \n",
    "     local_steps = np.ceil(10000/batch_size_chosen)\n",
    "     check_point = keras.callbacks.ModelCheckpoint(filepath=\"VGG-WallE\" + str(iteration) + \".h5\",\n",
    "                                              monitor=\"val_class_loss\",\n",
    "                                              mode=\"min\",\n",
    "                                              save_best_only=True,\n",
    "                                              )\n",
    "     model.compile(loss=[keras.losses.SparseCategoricalCrossentropy(from_logits=False),keras.losses.SparseCategoricalCrossentropy(from_logits=False),focal_loss],\n",
    "                  optimizer=keras.optimizers.RMSprop(), metrics=['accuracy'])\n",
    "     history = model.fit(x=train_x_new, y=[train_y_new, train_garbage, train_x_new], batch_size=batch_size_chosen, epochs=local_epoch, steps_per_epoch=local_steps, verbose=2,\n",
    "                        validation_data=(val_x_new, [val_y_new, val_garbage, val_x_new]),\n",
    "                        callbacks=[check_point, keras.callbacks.EarlyStopping(monitor='val_class_accuracy', patience=20)])\n",
    "    \n",
    "     return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_DCNN_model(upscaling_bool=False, global_pool=True, filters=[], fcd=[], pretrained=True)\n",
    "model_history, model_trained = train_model(77, model, batch_size_chosen=62)\n",
    "evaluate_model(model_history, model_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T07:03:38.753227Z",
     "iopub.status.busy": "2022-06-10T07:03:38.752954Z",
     "iopub.status.idle": "2022-06-10T07:03:40.597523Z",
     "shell.execute_reply": "2022-06-10T07:03:40.596371Z",
     "shell.execute_reply.started": "2022-06-10T07:03:38.753194Z"
    }
   },
   "outputs": [],
   "source": [
    "#upscaling_options = [False, True]\n",
    "#global_pooling = [True, False]\n",
    "#pretrained_opt = [False, True]\n",
    "#trainable_layers = [[16, 32, 64],[32, 64, 64, 128],[256, 256, 512],[8, 16, 32, 64],[8, 16, 32],[64, 128, 256],[32, 64, 128, 256, 512],[1024, 1024, 512]]\n",
    "#trainable_fcc = [[], [512], [1024], [512, 256], [1024, 512]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False True []\n",
      "Model: \"Ass1cQ2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 64, 64, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " block1_conv1 (Conv2D)          (None, 64, 64, 64)   1792        ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " block1_conv2 (Conv2D)          (None, 64, 64, 64)   36928       ['block1_conv1[0][0]']           \n",
      "                                                                                                  \n",
      " block1_pool (MaxPooling2D)     (None, 32, 32, 64)   0           ['block1_conv2[0][0]']           \n",
      "                                                                                                  \n",
      " block2_conv1 (Conv2D)          (None, 32, 32, 128)  73856       ['block1_pool[0][0]']            \n",
      "                                                                                                  \n",
      " block2_conv2 (Conv2D)          (None, 32, 32, 128)  147584      ['block2_conv1[0][0]']           \n",
      "                                                                                                  \n",
      " block2_pool (MaxPooling2D)     (None, 16, 16, 128)  0           ['block2_conv2[0][0]']           \n",
      "                                                                                                  \n",
      " block3_conv1 (Conv2D)          (None, 16, 16, 256)  295168      ['block2_pool[0][0]']            \n",
      "                                                                                                  \n",
      " block3_conv2 (Conv2D)          (None, 16, 16, 256)  590080      ['block3_conv1[0][0]']           \n",
      "                                                                                                  \n",
      " block3_conv3 (Conv2D)          (None, 16, 16, 256)  590080      ['block3_conv2[0][0]']           \n",
      "                                                                                                  \n",
      " block3_pool (MaxPooling2D)     (None, 8, 8, 256)    0           ['block3_conv3[0][0]']           \n",
      "                                                                                                  \n",
      " block4_conv1 (Conv2D)          (None, 8, 8, 512)    1180160     ['block3_pool[0][0]']            \n",
      "                                                                                                  \n",
      " block4_conv2 (Conv2D)          (None, 8, 8, 512)    2359808     ['block4_conv1[0][0]']           \n",
      "                                                                                                  \n",
      " block4_conv3 (Conv2D)          (None, 8, 8, 512)    2359808     ['block4_conv2[0][0]']           \n",
      "                                                                                                  \n",
      " block4_pool (MaxPooling2D)     (None, 4, 4, 512)    0           ['block4_conv3[0][0]']           \n",
      "                                                                                                  \n",
      " block5_conv1 (Conv2D)          (None, 4, 4, 512)    2359808     ['block4_pool[0][0]']            \n",
      "                                                                                                  \n",
      " block5_conv2 (Conv2D)          (None, 4, 4, 512)    2359808     ['block5_conv1[0][0]']           \n",
      "                                                                                                  \n",
      " block5_conv3 (Conv2D)          (None, 4, 4, 512)    2359808     ['block5_conv2[0][0]']           \n",
      "                                                                                                  \n",
      " block5_pool (MaxPooling2D)     (None, 2, 2, 512)    0           ['block5_conv3[0][0]']           \n",
      "                                                                                                  \n",
      " up_sampling2d (UpSampling2D)   (None, 4, 4, 512)    0           ['block5_pool[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 4, 4, 64)     294976      ['up_sampling2d[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 4, 4, 64)     36928       ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " up_sampling2d_1 (UpSampling2D)  (None, 8, 8, 64)    0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 8, 8, 32)     18464       ['up_sampling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 8, 8, 32)     9248        ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " up_sampling2d_2 (UpSampling2D)  (None, 16, 16, 32)  0           ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 16, 16, 16)   4624        ['up_sampling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 16, 16, 16)   2320        ['conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 512)         0           ['block5_pool[0][0]']            \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 16, 16, 3)    51          ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 512)          0           ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 64, 64, 3)    0           ['conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " class (Dense)                  (None, 257)          131841      ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " garbage (Dense)                (None, 4)            2052        ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " decoded (Conv2D)               (None, 64, 64, 3)    12          ['lambda[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 15,215,204\n",
      "Trainable params: 7,579,940\n",
      "Non-trainable params: 7,635,264\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/150\n",
      "162/162 - 177s - loss: 8.4736 - class_loss: 6.9731 - garbage_loss: 0.7359 - decoded_loss: 0.7646 - class_accuracy: 0.0564 - garbage_accuracy: 0.9464 - decoded_accuracy: 0.4448 - val_loss: 5.8607 - val_class_loss: 4.8510 - val_garbage_loss: 0.2333 - val_decoded_loss: 0.7764 - val_class_accuracy: 0.1114 - val_garbage_accuracy: 0.9516 - val_decoded_accuracy: 0.4337 - 177s/epoch - 1s/step\n",
      "Epoch 2/150\n",
      "162/162 - 173s - loss: 5.4024 - class_loss: 4.4036 - garbage_loss: 0.2362 - decoded_loss: 0.7626 - class_accuracy: 0.1464 - garbage_accuracy: 0.9507 - decoded_accuracy: 0.4652 - val_loss: 4.9457 - val_class_loss: 3.9514 - val_garbage_loss: 0.2182 - val_decoded_loss: 0.7761 - val_class_accuracy: 0.1931 - val_garbage_accuracy: 0.9516 - val_decoded_accuracy: 0.5377 - 173s/epoch - 1s/step\n",
      "Epoch 3/150\n",
      "162/162 - 174s - loss: 4.6822 - class_loss: 3.7028 - garbage_loss: 0.2175 - decoded_loss: 0.7620 - class_accuracy: 0.2246 - garbage_accuracy: 0.9498 - decoded_accuracy: 0.4698 - val_loss: 4.6053 - val_class_loss: 3.6125 - val_garbage_loss: 0.2170 - val_decoded_loss: 0.7757 - val_class_accuracy: 0.2530 - val_garbage_accuracy: 0.9518 - val_decoded_accuracy: 0.5157 - 174s/epoch - 1s/step\n",
      "Epoch 4/150\n",
      "162/162 - 174s - loss: 4.0536 - class_loss: 3.0999 - garbage_loss: 0.1920 - decoded_loss: 0.7617 - class_accuracy: 0.3164 - garbage_accuracy: 0.9503 - decoded_accuracy: 0.4934 - val_loss: 4.1314 - val_class_loss: 3.1572 - val_garbage_loss: 0.1990 - val_decoded_loss: 0.7751 - val_class_accuracy: 0.3216 - val_garbage_accuracy: 0.9525 - val_decoded_accuracy: 0.4758 - 174s/epoch - 1s/step\n",
      "Epoch 5/150\n",
      "162/162 - 174s - loss: 3.6534 - class_loss: 2.7238 - garbage_loss: 0.1663 - decoded_loss: 0.7633 - class_accuracy: 0.3866 - garbage_accuracy: 0.9546 - decoded_accuracy: 0.5149 - val_loss: 4.2081 - val_class_loss: 3.2306 - val_garbage_loss: 0.2023 - val_decoded_loss: 0.7752 - val_class_accuracy: 0.3151 - val_garbage_accuracy: 0.9516 - val_decoded_accuracy: 0.5011 - 174s/epoch - 1s/step\n",
      "Epoch 6/150\n",
      "162/162 - 176s - loss: 3.3698 - class_loss: 2.4486 - garbage_loss: 0.1610 - decoded_loss: 0.7601 - class_accuracy: 0.4365 - garbage_accuracy: 0.9541 - decoded_accuracy: 0.5115 - val_loss: 4.1930 - val_class_loss: 3.2037 - val_garbage_loss: 0.2145 - val_decoded_loss: 0.7748 - val_class_accuracy: 0.3441 - val_garbage_accuracy: 0.9530 - val_decoded_accuracy: 0.5477 - 176s/epoch - 1s/step\n",
      "Epoch 7/150\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/declanb/Documents/Machine Learning/CAB420_Ass2_DEC_FINAL_VGG.ipynb Cell 17'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/declanb/Documents/Machine%20Learning/CAB420_Ass2_DEC_FINAL_VGG.ipynb#ch0000017?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mstr\u001b[39m(upscaling) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(pooling) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(fcc_arr))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/declanb/Documents/Machine%20Learning/CAB420_Ass2_DEC_FINAL_VGG.ipynb#ch0000017?line=5'>6</a>\u001b[0m model \u001b[39m=\u001b[39m get_DCNN_model(upscaling_bool\u001b[39m=\u001b[39mupscaling, global_pool\u001b[39m=\u001b[39mpooling, filters\u001b[39m=\u001b[39m[], fcd\u001b[39m=\u001b[39mfcc_arr, pretrained\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/declanb/Documents/Machine%20Learning/CAB420_Ass2_DEC_FINAL_VGG.ipynb#ch0000017?line=6'>7</a>\u001b[0m model_history, model_trained \u001b[39m=\u001b[39m train_model(iteration_count, model, batch_size_chosen\u001b[39m=\u001b[39;49m\u001b[39m62\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/declanb/Documents/Machine%20Learning/CAB420_Ass2_DEC_FINAL_VGG.ipynb#ch0000017?line=7'>8</a>\u001b[0m evaluate_model(model_history, model_trained)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/declanb/Documents/Machine%20Learning/CAB420_Ass2_DEC_FINAL_VGG.ipynb#ch0000017?line=8'>9</a>\u001b[0m iteration_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[1;32m/home/declanb/Documents/Machine Learning/CAB420_Ass2_DEC_FINAL_VGG.ipynb Cell 15'\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(iteration, model, local_epoch, batch_size_chosen)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/declanb/Documents/Machine%20Learning/CAB420_Ass2_DEC_FINAL_VGG.ipynb#ch0000014?line=2'>3</a>\u001b[0m check_point \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mModelCheckpoint(filepath\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mVGG-WallE\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(iteration) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.h5\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/declanb/Documents/Machine%20Learning/CAB420_Ass2_DEC_FINAL_VGG.ipynb#ch0000014?line=3'>4</a>\u001b[0m                                          monitor\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mval_class_loss\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/declanb/Documents/Machine%20Learning/CAB420_Ass2_DEC_FINAL_VGG.ipynb#ch0000014?line=4'>5</a>\u001b[0m                                          mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/declanb/Documents/Machine%20Learning/CAB420_Ass2_DEC_FINAL_VGG.ipynb#ch0000014?line=5'>6</a>\u001b[0m                                          save_best_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/declanb/Documents/Machine%20Learning/CAB420_Ass2_DEC_FINAL_VGG.ipynb#ch0000014?line=6'>7</a>\u001b[0m                                          )\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/declanb/Documents/Machine%20Learning/CAB420_Ass2_DEC_FINAL_VGG.ipynb#ch0000014?line=7'>8</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m[keras\u001b[39m.\u001b[39mlosses\u001b[39m.\u001b[39mSparseCategoricalCrossentropy(from_logits\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m),keras\u001b[39m.\u001b[39mlosses\u001b[39m.\u001b[39mSparseCategoricalCrossentropy(from_logits\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m),focal_loss],\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/declanb/Documents/Machine%20Learning/CAB420_Ass2_DEC_FINAL_VGG.ipynb#ch0000014?line=8'>9</a>\u001b[0m              optimizer\u001b[39m=\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mRMSprop(), metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/declanb/Documents/Machine%20Learning/CAB420_Ass2_DEC_FINAL_VGG.ipynb#ch0000014?line=9'>10</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(x\u001b[39m=\u001b[39;49mtrain_x_new, y\u001b[39m=\u001b[39;49m[train_y_new, train_garbage, train_x_new], batch_size\u001b[39m=\u001b[39;49mbatch_size_chosen, epochs\u001b[39m=\u001b[39;49mlocal_epoch, steps_per_epoch\u001b[39m=\u001b[39;49mlocal_steps, verbose\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/declanb/Documents/Machine%20Learning/CAB420_Ass2_DEC_FINAL_VGG.ipynb#ch0000014?line=10'>11</a>\u001b[0m                    validation_data\u001b[39m=\u001b[39;49m(val_x_new, [val_y_new, val_garbage, val_x_new]),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/declanb/Documents/Machine%20Learning/CAB420_Ass2_DEC_FINAL_VGG.ipynb#ch0000014?line=11'>12</a>\u001b[0m                    callbacks\u001b[39m=\u001b[39;49m[check_point, keras\u001b[39m.\u001b[39;49mcallbacks\u001b[39m.\u001b[39;49mEarlyStopping(monitor\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mval_class_accuracy\u001b[39;49m\u001b[39m'\u001b[39;49m, patience\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m)])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/declanb/Documents/Machine%20Learning/CAB420_Ass2_DEC_FINAL_VGG.ipynb#ch0000014?line=13'>14</a>\u001b[0m \u001b[39mreturn\u001b[39;00m history, model\n",
      "File \u001b[0;32m~/Documents/Machine Learning/venv/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=61'>62</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=62'>63</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=63'>64</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Documents/Machine Learning/venv/lib/python3.9/site-packages/keras/engine/training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/keras/engine/training.py?line=1401'>1402</a>\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/keras/engine/training.py?line=1402'>1403</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/keras/engine/training.py?line=1403'>1404</a>\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/keras/engine/training.py?line=1404'>1405</a>\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[1;32m   <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/keras/engine/training.py?line=1405'>1406</a>\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/keras/engine/training.py?line=1406'>1407</a>\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m   <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/keras/engine/training.py?line=1407'>1408</a>\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/keras/engine/training.py?line=1408'>1409</a>\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/keras/engine/training.py?line=1409'>1410</a>\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/keras/engine/training.py?line=1410'>1411</a>\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/Documents/Machine Learning/venv/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py?line=147'>148</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py?line=148'>149</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py?line=149'>150</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py?line=150'>151</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py?line=151'>152</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Documents/Machine Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=911'>912</a>\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=913'>914</a>\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=914'>915</a>\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=916'>917</a>\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=917'>918</a>\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Documents/Machine Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=943'>944</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=944'>945</a>\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=945'>946</a>\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=946'>947</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=947'>948</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=948'>949</a>\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=949'>950</a>\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=950'>951</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/Documents/Machine Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=2449'>2450</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=2450'>2451</a>\u001b[0m   (graph_function,\n\u001b[1;32m   <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=2451'>2452</a>\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=2452'>2453</a>\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=2453'>2454</a>\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/Documents/Machine Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=1855'>1856</a>\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=1856'>1857</a>\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=1857'>1858</a>\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=1858'>1859</a>\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=1859'>1860</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=1860'>1861</a>\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=1861'>1862</a>\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=1862'>1863</a>\u001b[0m     args,\n\u001b[1;32m   <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=1863'>1864</a>\u001b[0m     possible_gradient_type,\n\u001b[1;32m   <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=1864'>1865</a>\u001b[0m     executing_eagerly)\n\u001b[1;32m   <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=1865'>1866</a>\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/Documents/Machine Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=494'>495</a>\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=495'>496</a>\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=496'>497</a>\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=497'>498</a>\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=498'>499</a>\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=499'>500</a>\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=500'>501</a>\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=501'>502</a>\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=502'>503</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=503'>504</a>\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=504'>505</a>\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=505'>506</a>\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=508'>509</a>\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=509'>510</a>\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/Documents/Machine Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/execute.py?line=51'>52</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/execute.py?line=52'>53</a>\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/execute.py?line=53'>54</a>\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/execute.py?line=54'>55</a>\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/execute.py?line=55'>56</a>\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     <a href='file:///home/declanb/Documents/Machine%20Learning/venv/lib/python3.9/site-packages/tensorflow/python/eager/execute.py?line=56'>57</a>\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#iteration_count = 0\n",
    "#for upscaling in upscaling_options:\n",
    "#    for pooling in global_pooling:\n",
    "#        for fcc_arr in trainable_fcc:\n",
    "#            print(str(upscaling) + \" \" + str(pooling) + \" \" + str(fcc_arr))\n",
    "#            model = get_DCNN_model(upscaling_bool=upscaling, global_pool=pooling, filters=[], fcd=fcc_arr, pretrained=True)\n",
    "#            model_history, model_trained = train_model(iteration_count, model, batch_size_chosen=62)\n",
    "#            evaluate_model(model_history, model_trained)\n",
    "#            iteration_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for upscaling in upscaling_options:\n",
    "    for pooling in global_pooling:\n",
    "        for filter_arr in trainable_layers:\n",
    "            for fcc_arr in trainable_fcc:\n",
    "                print(str(upscaling) + \" \" + str(pooling) + \" \" + str(filter_arr) + \" \" + str(fcc_arr))\n",
    "                model = get_DCNN_model(upscaling_bool=upscaling, global_pool=pooling, filters=filter_arr, fcd=fcc_arr, pretrained=False)\n",
    "                model_history, model_trained = train_model(iteration_count, model, batch_size_chosen=62)\n",
    "                evaluate_model(model_history, model_trained)\n",
    "                iteration_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#winner = 0\n",
    "#winner_value = 0\n",
    "#for layer_count in range(0, 234):\n",
    "#    model = update_trainable_layers(model, layer_count, norm_on=True)\n",
    "#    print(\"Layer Count: \" + str(layer_count))\n",
    "#    model_history, model = train_model(iteration_count, model, batch_size_chosen=32)\n",
    "#    print(\"Accuracy: \" + str(model_history.history['val_class_accuracy'][-1]))\n",
    "#    \n",
    "#    if (model_history.history['val_class_accuracy'][-1] > winner_value):\n",
    "#        winner = iteration_count\n",
    "#        winner_value = model_history.history['val_class_accuracy'][-1]\n",
    "#        print(\"###########################NEW WINNER###############################\")\n",
    "#    iteration_count += 1\n",
    "#evaluate_model(model_history, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-10T07:08:52.811559Z",
     "iopub.status.idle": "2022-06-10T07:08:52.812047Z",
     "shell.execute_reply": "2022-06-10T07:08:52.811810Z",
     "shell.execute_reply.started": "2022-06-10T07:08:52.811782Z"
    }
   },
   "outputs": [],
   "source": [
    "#absl-py>=0.15.0\n",
    "#argon2-cffi>=21.1.0\n",
    "#astunparse>=1.6.3\n",
    "#attrs>=21.2.0\n",
    "#backcall>=0.2.0\n",
    "#bleach>=4.1.0\n",
    "#cachetools>=4.2.4\n",
    "#certifi>=2021.10.8\n",
    "#cffi>=1.15.0\n",
    "#charset-normalizer>=2.0.7\n",
    "#clang>=5.0\n",
    "#cycler>=0.11.0\n",
    "#debugpy>=1.5.1\n",
    "#decorator>=5.1.0\n",
    "#defusedxml>=0.7.1\n",
    "#entrypoints>=0.3\n",
    "#flatbuffers>=1.12\n",
    "#gast>=0.4.0\n",
    "#gensim>=4.1.2\n",
    "#google-auth>=2.3.2\n",
    "#google-auth-oauthlib>=0.4.6\n",
    "#google-pasta>=0.2.0\n",
    "#grpcio>=1.41.1\n",
    "#h5py>=3.1.0\n",
    "#idna>=3.3\n",
    "#imageio>=2.10.1\n",
    "#ipykernel>=6.4.2\n",
    "#ipython>=7.28.0\n",
    "#ipython-genutils>=0.2.0\n",
    "#ipywidgets>=7.6.5\n",
    "#jedi>=0.18.0\n",
    "#Jinja2>=3.0.2\n",
    "#joblib>=1.1.0\n",
    "#jsonschema>=4.1.2\n",
    "#jupyter>=1.0.0\n",
    "#jupyter-client>=7.0.6\n",
    "#jupyter-console>=6.4.0\n",
    "#jupyter-core>=4.9.1\n",
    "#jupyterlab-pygments>=0.1.2\n",
    "#jupyterlab-widgets>=1.0.2\n",
    "#keras>=2.6.0\n",
    "#Keras-Preprocessing>=1.1.2\n",
    "#kiwisolver>=1.3.2\n",
    "#Markdown>=3.3.4\n",
    "#MarkupSafe>=2.0.1\n",
    "#matplotlib>=3.4.3\n",
    "#matplotlib-inline>=0.1.3\n",
    "#mistune>=0.8.4\n",
    "#nbclient>=0.5.4\n",
    "#nbconvert>=6.2.0\n",
    "#nbformat>=5.1.3\n",
    "#nest-asyncio>=1.5.1\n",
    "#networkx>=2.6.3\n",
    "#notebook>=6.4.5\n",
    "#numpy>=1.19.5\n",
    "#oauthlib>=3.1.1\n",
    "#opencv-python>=4.5.4.58\n",
    "#opt-einsum>=3.3.0\n",
    "#packaging>=21.0\n",
    "#pandas>=1.3.4\n",
    "#pandocfilters>=1.5.0\n",
    "#parso>=0.8.2\n",
    "#patsy>=0.5.2\n",
    "#pexpect>=4.8.0\n",
    "#pickleshare>=0.7.5\n",
    "#Pillow>=8.4.0\n",
    "#prometheus-client>=0.11.0\n",
    "#prompt-toolkit>=3.0.21\n",
    "#protobuf>=3.19.1\n",
    "#ptyprocess>=0.7.0\n",
    "#pyasn1>=0.4.8\n",
    "#pyasn1-modules>=0.2.8\n",
    "#pycparser>=2.20\n",
    "#pydot>=1.4.2\n",
    "#Pygments>=2.10.0\n",
    "#pyparsing>=3.0.3\n",
    "#pyrsistent>=0.18.0\n",
    "#python-dateutil>=2.8.2\n",
    "#pytz>=2021.3\n",
    "#PyWavelets>=1.1.1\n",
    "#pyzmq>=22.3.0\n",
    "#qtconsole>=5.1.1\n",
    "#QtPy>=1.11.2\n",
    "#requests>=2.26.0\n",
    "#requests-oauthlib>=1.3.0\n",
    "#rsa>=4.7.2\n",
    "#scikit-image>=0.18.3\n",
    "#scikit-learn>=1.0.1\n",
    "#scipy>=1.7.1\n",
    "#seaborn>=0.11.2\n",
    "#Send2Trash>=1.8.0\n",
    "#six>=1.15.0\n",
    "#smart-open>=5.2.1\n",
    "#statsmodels>=0.13.1\n",
    "#tensorboard>=2.7.0\n",
    "#tensorboard-data-server>=0.6.1\n",
    "#tensorboard-plugin-wit>=1.8.0\n",
    "#tensorflow>=2.6.0\n",
    "#tensorflow-estimator>=2.6.0\n",
    "#termcolor>=1.1.0\n",
    "#terminado>=0.12.1\n",
    "#testpath>=0.5.0\n",
    "#threadpoolctl>=3.0.0\n",
    "#tifffile>=2021.10.12\n",
    "#tornado>=6.1\n",
    "#tqdm>=4.64.0\n",
    "#traitlets>=5.1.1\n",
    "#typing-extensions>=3.7.4.3\n",
    "#urllib3>=1.26.7\n",
    "#wcwidth>=0.2.5\n",
    "#webencodings>=0.5.1\n",
    "#Werkzeug>=2.0.2\n",
    "#widgetsnbextension>=3.5.2\n",
    "#wrapt>=1.12.1\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d6d4960e980690e3ed5466399ce1a19c4d098217c487f99c6596fcd585196312"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
